# Copyright (c) 2010-2014, GEM Foundation.
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

"""
Core calculator functionality for computing stochastic event sets and ground
motion fields using the 'event-based' method.

Stochastic events sets (which can be thought of as collections of ruptures) are
computed iven a set of seismic sources and investigation time span (in years).

For more information on computing stochastic event sets, see
:mod:`openquake.hazardlib.calc.stochastic`.

One can optionally compute a ground motion field (GMF) given a rupture, a site
collection (which is a collection of geographical points with associated soil
parameters), and a ground shaking intensity model (GSIM).

For more information on computing ground motion fields, see
:mod:`openquake.hazardlib.calc.gmf`.
"""

import time
import random
import operator
import collections

import numpy.random

from django.db import transaction
from openquake.hazardlib.calc import gmf, filters
from openquake.hazardlib.imt import from_string

from openquake.commonlib import logictree
from openquake.commonlib.general import split_on_max_weight

from openquake.engine import logs, writer
from openquake.engine.calculators.hazard import general
from openquake.engine.calculators.hazard.classical import (
    post_processing as cls_post_proc)
from openquake.engine.db import models
from openquake.engine.utils import tasks
from openquake.engine.performance import (
    EnginePerformanceMonitor, LightMonitor, DummyMonitor)

# NB: beware of large caches
inserter = writer.CacheInserter(models.GmfData, 1000)


class RuptureData(object):
    """
    Containers for a main rupture and its copies. It has the attributes:

    :param r_sites:
        the collection of sites affected by the rupture
    :param rupture:
        an `openquake.hazardlib.source.rupture.
            ParametricProbabilisticRupture` instance
    :param rupid_seed_pairs:
        a list of pairs `(rupid, seed)` where `rupid` is the id of an
        `openquake.engine.db.models.SESRupture` instance and `seed` is
        an integer to be used as stochastic seed
    """
    def __init__(self, r_sites, rupture, rupid, rupid_seed_pairs):
        self.r_sites = r_sites
        self.rupture = rupture
        self.rupid = rupid
        self.rupid_seed_pairs = rupid_seed_pairs

    @property
    def weight(self):
        """
        The weight of a RuptureData object is the number of rupid_seed_pairs
        it contains, i.e. the number of GMFs it can generate.
        """
        return len(self.rupid_seed_pairs)

    def calc_gmf(self, imts, gsims, truncation_level, correl_model):
        """
        Compute the GMF generated by the given rupture on the given
        sites.
        """
        computer = gmf.GmfComputer(self.rupture, self.r_sites, imts, gsims,
                                   truncation_level, correl_model)
        for rupid, seed in self.rupid_seed_pairs:
            for (gsim_name, imt), gmvs in computer.compute(seed).iteritems():
                for site_id, gmv in zip(self.r_sites.sids, gmvs):
                    yield gsim_name, imt, site_id, rupid, gmv


# NB (MS): the approach used here will not work for non-poissonian models
def gmvs_to_haz_curve(gmvs, imls, invest_time, duration):
    """
    Given a set of ground motion values (``gmvs``) and intensity measure levels
    (``imls``), compute hazard curve probabilities of exceedance.

    :param gmvs:
        A list of ground motion values, as floats.
    :param imls:
        A list of intensity measure levels, as floats.
    :param float invest_time:
        Investigation time, in years. It is with this time span that we compute
        probabilities of exceedance.

        Another way to put it is the following. When computing a hazard curve,
        we want to answer the question: What is the probability of ground
        motion meeting or exceeding the specified levels (``imls``) in a given
        time span (``invest_time``).
    :param float duration:
        Time window during which GMFs occur. Another was to say it is, the
        period of time over which we simulate ground motion occurrences.

        NOTE: Duration is computed as the calculation investigation time
        multiplied by the number of stochastic event sets.

    :returns:
        Numpy array of PoEs (probabilities of exceedance).
    """
    # convert to numpy array and redimension so that it can be broadcast with
    # the gmvs for computing PoE values
    imls = numpy.array(imls).reshape((len(imls), 1))
    num_exceeding = numpy.sum(numpy.array(gmvs) >= imls, axis=1)
    poes = 1 - numpy.exp(- (invest_time / duration) * num_exceeding)
    return poes


def split(items, nblocks):
    """
    Produce blocks of items from a list of items, each one
    with a `.weight` attribute. The number of produced blocks
    will be close but not necessarily equal to `nblocks`.

    :params items: a sequence of weighted items
    :param nblocks: hint for the number of blocks to generate
    """
    assert nblocks > 0, nblocks
    pairs = [(item, item.weight) for item in items]
    weight = sum(w for (_, w) in pairs) / nblocks
    return split_on_max_weight(pairs, weight)


@tasks.oqtask
def compute_ruptures(
        job_id, sitecol, src_seeds, trt_model_id, gsims, task_no):
    """
    Celery task for the stochastic event set calculator.

    Samples logic trees and calls the stochastic event set calculator.

    Once stochastic event sets are calculated, results will be saved to the
    database. See :class:`openquake.engine.db.models.SESCollection`.

    Optionally (specified in the job configuration using the
    `ground_motion_fields` parameter), GMFs can be computed from each rupture
    in each stochastic event set. GMFs are also saved to the database.

    :param int job_id:
        ID of the currently running job.
    :param sitecol:
        a :class:`openquake.hazardlib.site.SiteCollection` instance
    :param src_seeds:
        List of pairs (source, seed)
    :params gsims:
        list of distinct GSIM instances
    :param task_no:
        an ordinal so that GMV can be collected in a reproducible order
    """
    # NB: all realizations in gsims correspond to the same source model
    trt_model = models.TrtModel.objects.get(pk=trt_model_id)
    ses_coll = models.SESCollection.objects.get(lt_model=trt_model.lt_model)

    hc = models.HazardCalculation.objects.get(oqjob=job_id)
    all_ses = range(1, hc.ses_per_logic_tree_path + 1)

    filter_sites_mon = LightMonitor(
        'filtering sites', job_id, compute_ruptures)
    generate_ruptures_mon = LightMonitor(
        'generating ruptures', job_id, compute_ruptures)
    filter_ruptures_mon = LightMonitor(
        'filtering ruptures', job_id, compute_ruptures)
    save_ruptures_mon = LightMonitor(
        'saving ruptures', job_id, compute_ruptures)

    # Compute and save stochastic event sets
    rnd = random.Random()
    num_distinct_ruptures = 0
    total_ruptures = 0
    rupture_data = []

    for src, seed in src_seeds:
        t0 = time.time()
        rnd.seed(seed)

        with filter_sites_mon:  # filtering sources
            s_sites = src.filter_sites_by_distance_to_source(
                hc.maximum_distance, sitecol
            ) if hc.maximum_distance else sitecol
            if s_sites is None:
                continue

        # the dictionary `ses_num_occ` contains [(ses, num_occurrences)]
        # for each occurring rupture for each ses in the ses collection
        ses_num_occ = collections.defaultdict(list)
        with generate_ruptures_mon:  # generating ruptures for the given source
            for rup_no, rup in enumerate(src.iter_ruptures(), 1):
                rup.rup_no = rup_no
                for ses_idx in all_ses:
                    numpy.random.seed(rnd.randint(0, models.MAX_SINT_32))
                    num_occurrences = rup.sample_number_of_occurrences()
                    if num_occurrences:
                        ses_num_occ[rup].append((ses_idx, num_occurrences))
                        total_ruptures += num_occurrences

        # NB: the number of occurrences is very low, << 1, so it is
        # more efficient to filter only the ruptures that occur, i.e.
        # to call sample_number_of_occurrences() *before* the filtering
        for rup in sorted(ses_num_occ, key=operator.attrgetter('rup_no')):
            with filter_ruptures_mon:  # filtering ruptures
                r_sites = filters.filter_sites_by_distance_to_rupture(
                    rup, hc.maximum_distance, s_sites
                    ) if hc.maximum_distance else s_sites
                if r_sites is None:
                    # ignore ruptures which are far away
                    del ses_num_occ[rup]  # save memory
                    continue

            # saving ses_ruptures
            ses_ruptures = []
            with save_ruptures_mon:
                # using a django transaction make the saving faster
                with transaction.commit_on_success(using='job_init'):
                    indices = r_sites.indices if len(r_sites) < len(sitecol) \
                        else None  # None means that nothing was filtered
                    prob_rup = models.ProbabilisticRupture.create(
                        rup, ses_coll, trt_model, indices)
                    for ses_idx, num_occurrences in ses_num_occ[rup]:
                        for occ_no in range(1, num_occurrences + 1):
                            rup_seed = rnd.randint(0, models.MAX_SINT_32)
                            ses_rup = models.SESRupture.create(
                                prob_rup, ses_idx, src.source_id,
                                rup.rup_no, occ_no, rup_seed)
                            ses_ruptures.append(ses_rup)

            # collecting ses_ruptures
            rupture_data.append(
                RuptureData(r_sites, rup, prob_rup.id,
                            [(ses_rup.id, ses_rup.seed)
                             for ses_rup in ses_ruptures]))

        # log calc_time per distinct rupture
        if ses_num_occ:
            num_ruptures = len(ses_num_occ)
            tot_ruptures = sum(num for rup in ses_num_occ
                               for ses, num in ses_num_occ[rup])
            logs.LOG.info(
                'job=%d, src=%s:%s, num_ruptures=%d, tot_ruptures=%d, '
                'num_sites=%d, calc_time=%fs', job_id, src.source_id,
                src.__class__.__name__, num_ruptures, tot_ruptures,
                len(s_sites), time.time() - t0)
            num_distinct_ruptures += num_ruptures

    if num_distinct_ruptures:
        logs.LOG.info('job=%d, task %d generated %d/%d ruptures',
                      job_id, task_no, num_distinct_ruptures, total_ruptures)
    filter_sites_mon.flush()
    generate_ruptures_mon.flush()
    filter_ruptures_mon.flush()
    save_ruptures_mon.flush()

    return rupture_data, trt_model_id, task_no


@tasks.oqtask
def compute_and_save_gmfs(job_id, sids, trt_model_id, rupture_data):
    """
    :param int job_id:
        ID of the currently running job
    :param int trt_model_id:
        ID of a TRTModel
    :param sids:
        numpy array of site IDs
    :param rupture_data:
        a list with the rupture data generated by the parent task
    """
    hc = models.HazardCalculation.objects.get(oqjob=job_id)
    params = dict(
        correl_model=hc.get_correl_model(),
        truncation_level=hc.truncation_level)
    imts = map(from_string, hc.intensity_measure_types)
    rlzs = models.TrtModel.objects.get(pk=trt_model_id).get_rlzs_by_gsim()
    gsims = [logictree.GSIM[gsim]() for gsim in rlzs]
    calc = GmfCalculator(params, imts, gsims, trt_model_id)
    calc.calc_gmfs(
        rupture_data, save=True,
        monitor=LightMonitor('', job_id, compute_and_save_gmfs))

    if hc.hazard_curves_from_gmfs:
        with EnginePerformanceMonitor(
                'hazard curves from gmfs', job_id, compute_and_save_gmfs):
            curves_by_gsim = calc.to_haz_curves(
                sids, hc.intensity_measure_types_and_levels,
                hc.investigation_time, hc.ses_per_logic_tree_path)
    else:
        curves_by_gsim = []

    return curves_by_gsim, trt_model_id, []


class GmfCalculator(object):
    """
    A class to store ruptures and then compute and save ground motion fields.
    """
    def __init__(self, params, imts, gsims, trt_model_id):
        """
        :param params:
            a dictionary of parameters with keys
            correl_model, truncation_level, maximum_distance
        :param imts:
            an ordered list of hazardlib intensity measure types
        :param int trt_model_id:
            the ID of a TRTModel instance
        """
        self.params = params
        self.imts = imts
        self.trt_model_id = trt_model_id
        self.gsims = gsims
        # the dictionay below is used in to_haz_curves
        self.gmvs_per_site = collections.defaultdict(list)

    def calc_gmfs(self, rupture_data, save=False, monitor=DummyMonitor()):
        """
        Compute the GMF generated by the given rupture on the given
        sites and collect the values in the dictionaries
        .gmvs_per_site and .ruptures_per_site.

        :param rupture_data:
            a list of RuptureData instances
        :param bool save:
            save the GMFs or not
        :param monitor:
            a monitor object
        """
        calc_mon = monitor.copy('computing gmvs')
        save_mon = monitor.copy('saving gmvs')
        inserter = writer.CacheInserter(models.GmfRupture, 1000)
        for rdata in rupture_data:
            with calc_mon:
                # gsim_name, imt, rupid -> gmvs
                gmf_dict = collections.defaultdict(list)
                for gsim_name, imt, site_id, rupid, gmv in rdata.calc_gmf(
                        self.imts, self.gsims, self.params['truncation_level'],
                        self.params['correl_model']):
                    gmf_dict[gsim_name, imt, rupid].append(gmv)
                    self.gmvs_per_site[gsim_name, imt, site_id].append(gmv)
            if save:
                with save_mon:
                    for (gsim_name, imt, rupid), gmvs in gmf_dict.iteritems():
                        inserter.add(
                            models.GmfRupture(
                                rupture_id=rupid, gsim=gsim_name, imt=str(imt),
                                ground_motion_field=gmvs))

        inserter.flush()
        calc_mon.flush()
        save_mon.flush()

    def to_haz_curves(self, sids, imtls, invest_time, num_ses):
        """
        Convert the gmf into hazard curves (by gsim and imt)

        :param sids: database ids of the given sites
        :param imtls: dictionary {IMT: intensity measure levels}
        :param invest_time: investigation time
        :param num_ses: number of Stochastic Event Sets
        """
        gmf = collections.defaultdict(dict)  # (gsim, imt) > {site_id: poes}
        for (gsim, imt, site_id), gmvs in self.gmvs_per_site.iteritems():
            gmf[gsim, imt][site_id] = gmvs_to_haz_curve(
                gmvs, imtls[str(imt)], invest_time, num_ses * invest_time)
        curves_by_gsim = []
        for gsim_obj in self.gsims:
            gsim = gsim_obj.__class__.__name__
            curves_by_imt = []
            for imt in self.imts:
                ground_motion_field = [gmf[gsim, imt].get(site_id, 0)
                                       for site_id in sids]
                curves_by_imt.append(ground_motion_field)
            curves_by_gsim.append((gsim, curves_by_imt))
        return curves_by_gsim


class EventBasedHazardCalculator(general.BaseHazardCalculator):
    """
    Probabilistic Event-Based hazard calculator. Computes stochastic event sets
    and (optionally) ground motion fields.
    """
    core_calc_task = compute_ruptures

    def task_arg_gen(self, _block_size=None):
        """
        Loop through realizations and sources to generate a sequence of
        task arg tuples. Each tuple of args applies to a single task.
        Yielded results are tuples of the form job_id, sources, ses, seeds
        (seeds will be used to seed numpy for temporal occurence sampling).
        """
        hc = self.hc
        rnd = random.Random()
        rnd.seed(hc.random_seed)
        for job_id, sitecol, block, lt_model, gsims, task_no in \
                super(EventBasedHazardCalculator, self).task_arg_gen():
            ss = [(src, rnd.randint(0, models.MAX_SINT_32))
                  for src in block]  # source, seed pairs
            yield job_id, sitecol, ss, lt_model, gsims, task_no

    def task_completed(self, task_result):
        """
        :param task_result:
            a triple (rupture_data, trt_model_id, task_no)

        If the parameter `ground_motion_fields` is set, compute and save
        the GMFs from the ruptures generated by the given task.
        """
        if not self.hc.ground_motion_fields:
            return  # do nothing
        rupture_data, trt_model_id, task_no = task_result
        if rupture_data:
            self.rupt_collector[trt_model_id].extend(rupture_data)
            self.num_ruptures[trt_model_id] += len(rupture_data)
            for rdata in rupture_data:
                for site_id in rdata.r_sites.sids:
                    self.site_ruptures[site_id].add(rdata.rupid)

    def post_execute(self):
        # first reset the number of ruptures to number of
        # actually generated ones
        for trt_model in models.TrtModel.objects.filter(
                lt_model__hazard_calculation=self.hc):
            trt_model.num_ruptures = self.num_ruptures.get(trt_model.id, 0)
            trt_model.save()

        if not self.hc.ground_motion_fields:
            return  # do nothing

        # save site_ruptures associations
        writer.CacheInserter.saveall(
            [models.SiteRuptures(site_id=site_id, rupture_ids=rupture_ids)
             for site_id, rupture_ids in self.site_ruptures.iteritems()])
        del self.site_ruptures  # save memory

        self.initialize_realizations()
        # create a Gmf output for each realization
        for rlz in self._get_realizations():
            output = models.Output.objects.create(
                oq_job=self.job,
                display_name='GMF rlz-%s' % rlz.id,
                output_type='gmf')
            models.Gmf.objects.create(output=output, lt_realization=rlz)

        # generate the GMFs and optionally the hazard curves too
        otm = tasks.OqTaskManager(compute_and_save_gmfs, logs.LOG.progress)
        task_no = 0
        sids = self.hc.site_collection.sids
        for trt_model_id, rupture_data in self.rupt_collector.iteritems():
            for rdata in split(rupture_data, 256):
                logs.LOG.info('Sending task #%s', task_no + 1)
                otm.submit(self.job.id, sids, trt_model_id, rdata)
                task_no += 1
        otm.aggregate_results(self.agg_curves, self.curves)

        # now save the curves, if any
        if self.curves:
            self.save_hazard_curves()

    def initialize_ses_db_records(self, lt_model):
        """
        Create :class:`~openquake.engine.db.models.Output`,
        :class:`~openquake.engine.db.models.SESCollection` and
        :class:`~openquake.engine.db.models.SES` "container" records for
        a single realization.

        Stochastic event set ruptures computed for this realization will be
        associated to these containers.

        NOTE: Many tasks can contribute ruptures to the same SES.
        """
        output = models.Output.objects.create(
            oq_job=self.job,
            display_name='SES Collection smlt-%d' % lt_model.ordinal,
            output_type='ses')

        ses_coll = models.SESCollection.objects.create(
            output=output, lt_model=lt_model, ordinal=lt_model.ordinal)

        return ses_coll

    def pre_execute(self):
        """
        Do pre-execution work. At the moment, this work entails:
        parsing and initializing sources, parsing and initializing the
        site model (if there is one), parsing vulnerability and
        exposure files, and generating logic tree realizations. (The
        latter piece basically defines the work to be done in the
        `execute` phase.)
        """
        super(EventBasedHazardCalculator, self).pre_execute()
        for lt_model in models.LtSourceModel.objects.filter(
                hazard_calculation=self.hc):
            self.initialize_ses_db_records(lt_model)

    def post_process(self):
        """
        If requested, perform additional processing of GMFs to produce hazard
        curves.
        """
        if not self.hc.hazard_curves_from_gmfs:
            return

        # If `mean_hazard_curves` is True and/or `quantile_hazard_curves`
        # has some value (not an empty list), do this additional
        # post-processing.
        if self.hc.mean_hazard_curves or self.hc.quantile_hazard_curves:
            with self.monitor('generating mean/quantile curves'):
                self.do_aggregate_post_proc()

        if self.hc.hazard_maps:
            with self.monitor('generating hazard maps'):
                self.parallelize(
                    cls_post_proc.hazard_curves_to_hazard_map_task,
                    cls_post_proc.hazard_curves_to_hazard_map_task_arg_gen(
                        self.job),
                    lambda res: None)
